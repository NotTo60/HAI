name: Provision Infra and Run Tests

on:
  push:
    branches: [main]

jobs:
  provision:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Ensure SSH key exists for Terraform
        run: |
          if [ ! -f terraform/id_rsa.pub ]; then
            ssh-keygen -t rsa -b 2048 -f terraform/id_rsa -N ""
          fi

      - name: Generate new SSH key for ec2-user
        run: |
          # Generate a new SSH key pair specifically for ec2-user
          ssh-keygen -t rsa -b 2048 -f terraform/ec2_user_rsa -N ""
          echo "Generated new SSH key pair for ec2-user"
          echo "Public key:"
          cat terraform/ec2_user_rsa.pub

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Verify Terraform Installation
        run: |
          echo "=== TERRAFORM INSTALLATION VERIFICATION ==="
          echo "Terraform version:"
          terraform version
          echo "Terraform location:"
          which terraform
          echo "PATH:"
          echo $PATH
          echo "=== END VERIFICATION ==="

      - name: Validate AWS Credentials
        run: |
          echo "Validating AWS credentials..."
          aws sts get-caller-identity
          echo "AWS credentials are valid!"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Remove existing AWS key pair if present
        run: |
          aws ec2 delete-key-pair --key-name hai-ci-key || true
          aws ec2 delete-key-pair --key-name hai-ci-ec2-user-key || true
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Clean up conflicting subnets
        run: |
          echo "Checking for existing subnets that might conflict..."
          # Get VPC ID
          VPC_ID=$(aws ec2 describe-vpcs --query 'Vpcs[0].VpcId' --output text 2>/dev/null || echo "")
          if [ ! -z "$VPC_ID" ]; then
            echo "Found VPC: $VPC_ID"
            # List subnets in the VPC that might conflict with our new CIDR
            SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[?CidrBlock==`10.0.200.0/24`].SubnetId' --output text 2>/dev/null || echo "")
            if [ ! -z "$SUBNETS" ]; then
              echo "Found conflicting subnet(s): $SUBNETS"
              echo "Deleting conflicting subnets..."
              for SUBNET in $SUBNETS; do
                aws ec2 delete-subnet --subnet-id $SUBNET || echo "Failed to delete subnet $SUBNET"
              done
            else
              echo "No conflicting subnets found with CIDR 10.0.200.0/24"
            fi
            
            # Also check for any subnets in the 10.0.x.x range that might overlap
            echo "Checking for any subnets in 10.0.x.x range..."
            ALL_SUBNETS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[?starts_with(CidrBlock, `10.0.`)].{SubnetId:SubnetId,CidrBlock:CidrBlock}' --output json 2>/dev/null || echo "[]")
            echo "Existing subnets in 10.0.x.x range: $ALL_SUBNETS"
            
            # If there are any subnets in the 10.0.x.x range, delete them all to avoid conflicts
            SUBNET_IDS=$(aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" --query 'Subnets[?starts_with(CidrBlock, `10.0.`)].SubnetId' --output text 2>/dev/null || echo "")
            if [ ! -z "$SUBNET_IDS" ]; then
              echo "Found existing subnets in 10.0.x.x range, deleting all to avoid conflicts: $SUBNET_IDS"
              for SUBNET_ID in $SUBNET_IDS; do
                echo "Deleting subnet: $SUBNET_ID"
                aws ec2 delete-subnet --subnet-id $SUBNET_ID || echo "Failed to delete subnet $SUBNET_ID"
              done
            fi
          else
            echo "No VPC found"
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Terraform Init and Apply
        run: |
          echo "=== TERRAFORM EXECUTION DEBUG ==="
          echo "Current directory: $(pwd)"
          echo "Terraform directory contents:"
          ls -la terraform/
          echo "Terraform executable:"
          which terraform
          echo "Terraform version:"
          terraform version
          echo "=== STARTING TERRAFORM ==="
          cd terraform
          echo "Changed to terraform directory: $(pwd)"
          terraform init
          terraform apply -auto-approve
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}

      - name: Output VM IPs
        id: tf_outputs
        run: |
          echo "=== TERRAFORM OUTPUT DEBUG ==="
          echo "Current directory: $(pwd)"
          echo "Terraform executable:"
          which terraform
          echo "Terraform version:"
          terraform version
          echo "=== GETTING TERRAFORM OUTPUTS ==="
          cd terraform
          echo "Changed to terraform directory: $(pwd)"
          echo "linux_ip=$(terraform output -raw linux_ip)" >> $GITHUB_OUTPUT
          echo "windows_ip=$(terraform output -raw windows_ip)" >> $GITHUB_OUTPUT
          echo "Linux instance created with IP: $(terraform output -raw linux_ip)"
          echo "Windows instance created with IP: $(terraform output -raw windows_ip)"

      - name: Upload SSH key for later jobs
        uses: actions/upload-artifact@v4
        with:
          name: linux_ssh_key
          path: terraform/ec2_user_rsa

    outputs:
      linux_ip: ${{ steps.tf_outputs.outputs.linux_ip }}
      windows_ip: ${{ steps.tf_outputs.outputs.windows_ip }}

  wait_for_ready:
    needs: provision
    runs-on: ubuntu-latest
    steps:
      - name: Download SSH key
        uses: actions/download-artifact@v4
        with:
          name: linux_ssh_key

      - name: Debug instance and network status
        run: |
          echo "=== INSTANCE AND NETWORK DEBUG ==="
          echo "Linux IP: ${{ needs.provision.outputs.linux_ip }}"
          echo "Windows IP: ${{ needs.provision.outputs.windows_ip }}"
          
          # Check instance status
          echo "Checking Linux instance status..."
          aws ec2 describe-instances \
            --filters "Name=ip-address,Values=${{ needs.provision.outputs.linux_ip }}" \
            --query 'Reservations[].Instances[].{InstanceId:InstanceId,State:State.Name,PublicIP:PublicIpAddress,PrivateIP:PrivateIpAddress}' \
            --output table || echo "Failed to get instance info"
          
          # Check security groups
          echo "Checking security groups..."
          aws ec2 describe-instances \
            --filters "Name=ip-address,Values=${{ needs.provision.outputs.linux_ip }}" \
            --query 'Reservations[].Instances[].SecurityGroups[].{GroupId:GroupId,GroupName:GroupName}' \
            --output table || echo "Failed to get security group info"
          
          # Check route table
          echo "Checking route table..."
          aws ec2 describe-route-tables \
            --filters "Name=association.subnet-id,Values=*" \
            --query 'RouteTables[].Routes[?GatewayId!=`null`].{GatewayId:GatewayId,DestinationCidrBlock:DestinationCidrBlock}' \
            --output table || echo "Failed to get route table info"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Wait for Linux SSH
        run: |
          chmod 600 ec2_user_rsa
          echo "=== SSH CONNECTIVITY DEBUG ==="
          echo "Target IP: ${{ needs.provision.outputs.linux_ip }}"
          echo "SSH key exists: $(ls -la ec2_user_rsa)"
          echo "SSH key permissions: $(ls -la ec2_user_rsa | awk '{print $1}')"
          
          # Wait for instance to be ready (longer timeout)
          echo "Waiting for instance to be ready..."
          for attempt in {1..30}; do
            echo "Attempt $attempt/30: Testing SSH connectivity..."
            
            # Test basic connectivity first
            if nc -z -w 5 ${{ needs.provision.outputs.linux_ip }} 22; then
              echo "Port 22 is reachable, testing SSH..."
              
              # Try to detect the correct username by trying common ones
              for username in ubuntu ec2-user admin; do
                echo "Trying username: $username"
                
                # Test SSH with timeout and verbose output
                if timeout 10 ssh -v -o StrictHostKeyChecking=no -o ConnectTimeout=5 -o BatchMode=yes -i ec2_user_rsa $username@${{ needs.provision.outputs.linux_ip }} "echo ready" 2>&1; then
                  echo "Linux SSH is ready with username: $username!"
                  echo "DETECTED_USERNAME=$username" >> $GITHUB_ENV
                  exit 0
                else
                  echo "Username $username failed, trying next..."
                fi
              done
            else
              echo "Port 22 not reachable yet..."
            fi
            
            echo "Waiting 10 seconds before next attempt..."
            sleep 10
          done
          
          echo "ERROR: Linux SSH not reachable after 5 minutes."
          echo "Final connectivity test:"
          nc -zv ${{ needs.provision.outputs.linux_ip }} 22 || echo "Port 22 still not reachable"
          exit 1

      - name: Set output for detected username
        id: set_username
        run: echo "detected_username=${DETECTED_USERNAME}" >> $GITHUB_OUTPUT

      - name: Wait for Windows RDP/SMB
        run: |
          for i in {1..3}; do
            if nc -z -v ${{ needs.provision.outputs.windows_ip }} 445; then
              echo "Windows SMB is ready!"; exit 0;
            fi
            echo "Waiting for SMB..."; sleep 10;
          done
          echo "ERROR: Windows SMB not reachable after 30 seconds."; exit 1

    outputs:
      detected_username: ${{ steps.set_username.outputs.detected_username }}

  run_tests:
    needs: [wait_for_ready, provision]
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Download SSH key
        uses: actions/download-artifact@v4
        with:
          name: linux_ssh_key

      - name: Set SSH key permissions
        run: |
          chmod 600 ec2_user_rsa
          echo "SSH key permissions set"

      - name: Test SSH to Linux
        working-directory: ${{ github.workspace }}
        run: bash tests/test_linux_ssh_connectivity.sh ${{ needs.provision.outputs.linux_ip }} ${{ needs.wait_for_ready.outputs.detected_username }}
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Test SMB to Windows
        shell: pwsh
        working-directory: ${{ github.workspace }}
        run: tests/test_windows_smb_connectivity.ps1 -TargetIP ${{ needs.provision.outputs.windows_ip }}
        env:
          PYTHONPATH: ${{ github.workspace }}

      - name: Run tests
        working-directory: ${{ github.workspace }}
        env:
          # Linux host IP comes from Terraform outputs, not secrets
          TEST_LINUX_HOST: ${{ needs.provision.outputs.linux_ip }}
          TEST_LINUX_USER: ${{ needs.wait_for_ready.outputs.detected_username }}
          TEST_LINUX_SSH_KEY: ${{ github.workspace }}/ec2_user_rsa
          # Windows credentials still need secrets
          TEST_WINDOWS_HOST: ${{ needs.provision.outputs.windows_ip }}
          TEST_WINDOWS_USER: Administrator
          TEST_WINDOWS_PASS: ${{ secrets.TEST_WINDOWS_PASS }}
        run: |
          # Debug information before installation
          echo "=== PRE-INSTALLATION DEBUG ==="
          echo "PYTHONPATH=$PYTHONPATH"
          echo "Current directory: $(pwd)"
          echo "Files in current directory: $(ls -la)"
          echo "Files in core directory: $(ls -la core/)"
          echo "SSH key exists: $(ls -la ec2_user_rsa)"
          echo "Python path: $(python -c 'import sys; print("\\n".join(sys.path))')"
          
          # Install dependencies first
          echo "=== INSTALLING DEPENDENCIES ==="
          pip install paramiko>=2.7.0 impacket>=0.9.0 pytest>=6.0.0 pyyaml>=5.0 cryptography>=3.0 bcrypt>=3.0 pynacl>=1.0
          
          # Try multiple installation methods
          echo "=== INSTALLING PACKAGE ==="
          
          # Method 1: Install in development mode
          pip install -e . || echo "pip install -e . failed"
          
          # Method 2: Add current directory to PYTHONPATH
          export PYTHONPATH="${GITHUB_WORKSPACE}:${PYTHONPATH:-}"
          
          # Method 3: Try installing with pip install .
          pip install . || echo "pip install . failed"
          
          echo "=== POST-INSTALLATION DEBUG ==="
          echo "PYTHONPATH=$PYTHONPATH"
          echo "Python path: $(python -c 'import sys; print("\\n".join(sys.path))')"
          
          # Test imports
          echo "=== TESTING IMPORTS ==="
          python -c "import sys; print('Python executable:', sys.executable)"
          python -c "import core; print('Core module imported successfully')" || echo "Core import failed"
          python -c "from core.connection_manager import connect_with_fallback; print('connection_manager imported successfully')" || echo "connection_manager import failed"
          
          # List installed packages
          echo "=== INSTALLED PACKAGES ==="
          pip list | grep -E "(hai|core)" || echo "No hai or core packages found"
          
          # Run tests with verbose output
          echo "=== RUNNING TESTS ==="
          python -m pytest tests/test_integration_real_servers.py -v --tb=short

      - name: Upload test logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-logs
          path: logs/

  cleanup:
    needs: [provision, run_tests]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Destroy Terraform-managed infrastructure
        run: |
          echo "=== STARTING TERRAFORM CLEANUP ==="
          cd terraform
          echo "Changed to terraform directory: $(pwd)"
          
          # Initialize Terraform
          echo "Initializing Terraform..."
          terraform init || echo "Terraform init failed, continuing..."
          
          # Try to destroy with Terraform first
          echo "Attempting Terraform destroy..."
          terraform destroy -auto-approve || echo "Terraform destroy failed, continuing with manual cleanup..."
          
          echo "=== TERRAFORM CLEANUP COMPLETED ==="
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Targeted cleanup - Remove workflow-specific key pairs
        run: |
          echo "=== TARGETED CLEANUP - KEY PAIRS ==="
          echo "Removing only workflow-specific key pairs..."
          
          # Only delete our specific key pairs
          aws ec2 delete-key-pair --key-name hai-ci-key 2>/dev/null || echo "Failed to delete hai-ci-key, continuing..."
          aws ec2 delete-key-pair --key-name hai-ci-ec2-user-key 2>/dev/null || echo "Failed to delete hai-ci-ec2-user-key, continuing..."
          
          echo "Key pair cleanup completed"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Targeted cleanup - Delete workflow-specific EC2 instances
        run: |
          echo "=== TARGETED CLEANUP - EC2 INSTANCES ==="
          echo "Checking for workflow-specific EC2 instances..."
          
          # Get list of instances tagged with our workflow identifier
          instance_list=$(aws ec2 describe-instances \
            --filters "Name=tag:ManagedBy,Values=hai-ci-workflow" \
            --query 'Reservations[].Instances[].InstanceId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$instance_list" ]; then
            echo "Found workflow-specific instances to terminate: $instance_list"
            for instance_id in $instance_list; do
              echo "Terminating workflow instance $instance_id"
              aws ec2 terminate-instances --instance-ids $instance_id 2>/dev/null || echo "Failed to terminate instance $instance_id, continuing..."
            done
          else
            echo "No workflow-specific instances found"
          fi
          
          echo "EC2 instance cleanup completed"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Targeted cleanup - Delete workflow-specific security groups
        run: |
          echo "=== TARGETED CLEANUP - SECURITY GROUPS ==="
          echo "Checking for workflow-specific security groups..."
          
          # Get list of security groups tagged with our workflow identifier
          sg_list=$(aws ec2 describe-security-groups \
            --filters "Name=tag:ManagedBy,Values=hai-ci-workflow" \
            --query 'SecurityGroups[].GroupId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$sg_list" ]; then
            echo "Found workflow-specific security groups to delete: $sg_list"
            for sg_id in $sg_list; do
              echo "Deleting workflow security group $sg_id"
              aws ec2 delete-security-group --group-id $sg_id 2>/dev/null || echo "Failed to delete security group $sg_id, continuing..."
            done
          else
            echo "No workflow-specific security groups found"
          fi
          
          echo "Security group cleanup completed"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Targeted cleanup - Delete workflow-specific VPCs and related resources
        run: |
          echo "=== TARGETED CLEANUP - VPCS ==="
          echo "Cleaning up only workflow-specific VPCs..."
          
          # Get list of VPCs tagged with our workflow identifier
          vpc_list=$(aws ec2 describe-vpcs \
            --filters "Name=tag:ManagedBy,Values=hai-ci-workflow" \
            --query 'Vpcs[].VpcId' \
            --output text 2>/dev/null || echo "")
          
          if [ -n "$vpc_list" ]; then
            echo "Found workflow-specific VPCs to delete: $vpc_list"
            for vpc_id in $vpc_list; do
              echo "Processing workflow VPC $vpc_id"
              
              # Detach and delete internet gateways tagged with our workflow
              igw_list=$(aws ec2 describe-internet-gateways \
                --filters "Name=attachment.vpc-id,Values=$vpc_id" "Name=tag:ManagedBy,Values=hai-ci-workflow" \
                --query 'InternetGateways[].InternetGatewayId' \
                --output text 2>/dev/null || echo "")
              if [ -n "$igw_list" ]; then
                for igw_id in $igw_list; do
                  echo "Detaching workflow IGW $igw_id from VPC $vpc_id"
                  aws ec2 detach-internet-gateway --internet-gateway-id $igw_id --vpc-id $vpc_id 2>/dev/null || echo "Failed to detach IGW $igw_id, continuing..."
                  echo "Deleting workflow IGW $igw_id"
                  aws ec2 delete-internet-gateway --internet-gateway-id $igw_id 2>/dev/null || echo "Failed to delete IGW $igw_id, continuing..."
                done
              fi
              
              # Delete subnets tagged with our workflow
              subnet_list=$(aws ec2 describe-subnets \
                --filters "Name=vpc-id,Values=$vpc_id" "Name=tag:ManagedBy,Values=hai-ci-workflow" \
                --query 'Subnets[].SubnetId' \
                --output text 2>/dev/null || echo "")
              if [ -n "$subnet_list" ]; then
                for subnet_id in $subnet_list; do
                  echo "Deleting workflow subnet $subnet_id"
                  aws ec2 delete-subnet --subnet-id $subnet_id 2>/dev/null || echo "Failed to delete subnet $subnet_id, continuing..."
                done
              fi
              
              # Delete route tables tagged with our workflow
              rt_list=$(aws ec2 describe-route-tables \
                --filters "Name=vpc-id,Values=$vpc_id" "Name=tag:ManagedBy,Values=hai-ci-workflow" \
                --query 'RouteTables[].RouteTableId' \
                --output text 2>/dev/null || echo "")
              if [ -n "$rt_list" ]; then
                for rt_id in $rt_list; do
                  echo "Deleting workflow route table $rt_id"
                  aws ec2 delete-route-table --route-table-id $rt_id 2>/dev/null || echo "Failed to delete route table $rt_id, continuing..."
                done
              fi
              
              # Finally delete the workflow VPC
              echo "Deleting workflow VPC $vpc_id"
              aws ec2 delete-vpc --vpc-id $vpc_id 2>/dev/null || echo "Failed to delete VPC $vpc_id, continuing..."
            done
          else
            echo "No workflow-specific VPCs found"
          fi
          
          echo "VPC cleanup completed"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: us-east-1

      - name: Cleanup summary
        run: |
          echo "=== TARGETED CLEANUP SUMMARY ==="
          echo "All targeted cleanup operations completed!"
          echo "Only resources tagged with 'ManagedBy=hai-ci-workflow' were cleaned up."
          echo "Existing resources not created by this workflow were preserved."
